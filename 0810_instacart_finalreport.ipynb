{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data\n",
    "prior_df = pd.read_csv('order_products__prior.csv', dtype={'order_id': np.uint32,\n",
    "           'product_id': np.uint16, 'reordered': np.uint8, 'add_to_cart_order': np.uint8})\n",
    "\n",
    "train_df = pd.read_csv('order_products__train.csv', dtype={'order_id': np.uint32,\n",
    "           'product_id': np.uint16, 'reordered': np.int8, 'add_to_cart_order': np.uint8 })\n",
    "\n",
    "orders_df = pd.read_csv('orders.csv', dtype={'order_hour_of_day': np.uint8,\n",
    "           'order_number': np.uint8, 'order_id': np.uint32, 'user_id': np.uint32,\n",
    "           'order_dow': np.uint8, 'days_since_prior_order': np.float16})\n",
    "\n",
    "products_df = pd.read_csv('products.csv', dtype={'product_id': np.uint16,\n",
    "            'aisle_id': np.uint8, 'department_id': np.uint8},\n",
    "             usecols=['product_id', 'aisle_id', 'department_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining prior and train data to do feature engineering. They will be separated later for train and test splitting\n",
    "prior_train_df=pd.concat([prior_df,train_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prior    3214874\n",
       "train     131209\n",
       "test       75000\n",
       "Name: eval_set, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the number of orders in each set. \n",
    "orders_df[\"eval_set\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eval_set\n",
       "prior    206209\n",
       "test      75000\n",
       "train    131209\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the number of unique users in each set. Note that prior has all the users. Some of these users are present in the train set\n",
    "# While the rest are present in the test set. \n",
    "orders_df.groupby(\"eval_set\")[\"user_id\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently our dataframe contains data in the below format\n",
    "\n",
    "| User  | Order_number   | Product   | Reordered   |\n",
    "|---|---|---|---|\n",
    "| 1  | 1  |  A | 0  |\n",
    "| 1  |  1 |  B |  0 | \n",
    "| 1  |  1 |  C |  0 |\n",
    "| 1  | 2  |  B | 1  |\n",
    "| 1  | 2  |  D | 0  |\n",
    "\n",
    "\n",
    "But because our objective is to predict which of the products a customer has bought in the past will be reordered in his latest order, it will be better to have all the products which a customer has bought in the past aggregated and predict 0 or 1 against each product . For instance let us suppose data in the following format \n",
    "\n",
    "| User  | Order_number   | Product   | Ordered(1)/Not Ordered (0)|\n",
    "|---|---|---|---|\n",
    "| 1  | 1  |  A | 1  |\n",
    "| 1  |  1 |  B | 1 | \n",
    "| 1  |  1 |  C |  1 |\n",
    "| 1  |  1 |  D |  0 |\n",
    "| 1  | 2  |  A | 0  |\n",
    "| 1  | 2  |  B | 1  |\n",
    "| 1  | 2  |  C | 0  |\n",
    "| 1  | 2  |  D | 1  |\n",
    "\n",
    "\n",
    "\n",
    "The aggregate of all the products customer has ordered in his history is {A,B,C,D}\n",
    "\n",
    "Lets say the above customer's latest order is order_number = 3 then our objective will be to predict ordered or not-ordered for the products A,B,C,D\n",
    "\n",
    "| User  | Order_number   | Product   | Ordered(1)/Not Ordered (0)|\n",
    "|---|---|---|---|\n",
    "| 1  | 1  |  A | 1  |\n",
    "| 1  |  1 |  B | 1 | \n",
    "| 1  |  1 |  C |  1 |\n",
    "| 1  |  1 |  D |  0 |\n",
    "| 1  | 2  |  A | 0  |\n",
    "| 1  | 2  |  B | 1  |\n",
    "| 1  | 2  |  C | 0  |\n",
    "| 1  | 2  |  D | 1  |\n",
    "| 1  | 3  |  A | ?  |\n",
    "| 1  | 3  |  B | ?  |\n",
    "| 1  | 3  |  C | ?  |\n",
    "| 1  | 3  |  D | ?  |\n",
    "\n",
    "So instead of focusing on the reordered variable, for a particular customer's latest order we will try to predict either 0 or 1 against each product which the customer has ever ordered in the past. Now let us build the above user-product table as outlined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_prior_train_df=prior_train_df.merge(orders_df,on=\"order_id\")\n",
    "up_user_products=orders_prior_train_df[[\"user_id\",\"product_id\"]].drop_duplicates()\n",
    "\n",
    "#Aggregate all the products which a customer has ordered into a set\n",
    "temp=orders_prior_train_df.groupby([\"user_id\",\"order_id\"],as_index=False).agg({\"product_id\":(lambda x:set(x))})\n",
    "temp=temp.merge(up_user_products,on=\"user_id\").rename(columns={\"product_id_x\":\"order_id_products\",\"product_id_y\":\"product_id\"})\n",
    "\n",
    "#Create table with either 0 or 1 against each user, product id (product id from the aggregated set) as outlined in discussion above\n",
    "temp_df=temp.head(1)\n",
    "temp_df[\"in_cart\"]=2\n",
    "temp_df[\"user_id\"]=0\n",
    "temp_df=temp_df.drop(\"order_id_products\",axis=1)\n",
    "\n",
    "#Please note the following steps takes around 40GB of RAM\n",
    "chunksize=int(temp.shape[0]/20)\n",
    "i=0\n",
    "for start in range(0,temp.shape[0],chunksize):\n",
    "    i=i+1\n",
    "    print(i)\n",
    "    df_subset=temp.iloc[start:start+chunksize]\n",
    "    df_subset[\"in_cart\"]=(df_subset.apply(lambda row: row['product_id'] in row['order_id_products'], axis=1).astype(int))\n",
    "    df_subset=df_subset.drop(\"order_id_products\",axis=1)\n",
    "    temp_df=pd.concat([temp_df,df_subset])\n",
    "    \n",
    "temp=orders_prior_train_df[[\"order_id\",\"u_order_number\"]].drop_duplicates()\n",
    "temp_df=temp_df.merge(temp,on=\"order_id\")\n",
    "\n",
    "#Pickle this dataset for subsequent use\n",
    "temp_df.to_pickle(\"temp_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "For our problem, it is natural to think about three types of features: 1) <b> User-related features </b> : Features which are specific to the user in question 2) <b> Product-related features </b>: Features which are specific to a particular product 3) <b> User x Product related features </b>: Features which capture or quantify trends related to a particular user-product combination\n",
    "\n",
    "<b> Let us start by building user-related features </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting total number of products (\"total_user_purchases\") ordered by a customer in all his orders (total number, not unique items)\n",
    "temp=orders_prior_train_df.groupby(\"user_id\")[\"add_to_cart_order\"].count().reset_index().rename(columns={\"add_to_cart_order\":\"total_user_purchases\"})\n",
    "\n",
    "# Adding the \"total_user_purchases\" feature to orders_prior_train_df\n",
    "orders_prior_train_df=orders_prior_train_df.merge(temp,on=\"user_id\")\n",
    "\n",
    "# Calculating the total number of orders (\"number_of_orders\") for each customer\n",
    "temp = orders_prior_train_df.groupby(\"user_id\")[\"order_number\"].nunique().reset_index().rename(columns={\"order_number\":\"number_of_orders\"})\n",
    "\n",
    "# Adding the \"number_of_orders\" feature to orders_prior_train_df\n",
    "orders_prior_train_df=orders_prior_train_df.merge(temp,on=\"user_id\")\n",
    "\n",
    "#Calculating the average cart size (\"avg_cart_size\") of each customer\n",
    "orders_prior_train_df[\"avg_cart_size\"]=orders_prior_train_df[\"total_user_purchases\"]/orders_prior_train_df[\"number_of_orders\"]\n",
    "\n",
    "#Calculate average number of days between orders for a customer to get the frequency of ordering \n",
    "temp=orders_df.groupby(\"user_id\")[\"days_since_prior_order\"].mean().reset_index().rename(columns={\"days_since_prior_order\":\"average_days_since_prior_order\"})\n",
    "\n",
    "# Adding the \"average_days_since_prior_order\" feature to orders_prior_train_df\n",
    "orders_prior_train_df=orders_prior_train_df.merge(temp,on=\"user_id\")\n",
    "\n",
    "# Calculate how long a user has been a customer (\"customer_time\") #CHECK THIS\n",
    "temp=orders_df.groupby(\"user_id\")[\"days_since_prior_order\"].sum().reset_index().rename(columns={\"days_since_prior_order\":\"customer_time\"})\n",
    "\n",
    "# Adding the \"customer_time\" feature to orders_prior_train_df\n",
    "orders_prior_train_df=orders_prior_train_df.merge(temp,on=\"user_id\")\n",
    "\n",
    "#Calculating the average day of the week (\"average_dow\") on which a customer places an order\n",
    "temp=orders_df.groupby(\"user_id\")[\"order_dow\"].mean().reset_index().rename(columns={\"order_dow\":\"average_dow\"})\n",
    "\n",
    "# Adding the \"average_dow\" feature to orders_prior_train_df \n",
    "orders_prior_train_df=orders_prior_train_df.merge(temp,on=\"user_id\")\n",
    "\n",
    "#Calculating the average hour of the day (\"average_order_hour_of_day\") on which a customer places an order\n",
    "temp=orders_df.groupby(\"user_id\")[\"order_hour_of_day\"].mean().reset_index().rename(columns={\"order_hour_of_day\":\"average_order_hour_of_day\"})\n",
    "\n",
    "# Adding the \"average_order_hour_of_day\" feature to orders_prior_train_df \n",
    "orders_prior_train_df=orders_prior_train_df.merge(temp,on=\"user_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Now build product-related features </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the total times a product is ordered (\"total_product_orders\")\n",
    "temp=orders_prior_train_df[\"product_id\"].value_counts().reset_index().rename(columns={\"index\":\"product_id\",\"product_id\":\"total_product_orders\"})\n",
    "\n",
    "# Adding the \"total_product_orders\" feature to orders_prior_train_df \n",
    "orders_prior_train_df=orders_prior_train_df.merge(temp,on=\"product_id\")\n",
    "\n",
    "# calculate the average number of days before which users order a particular product (\"average_dspo_product\") #CHECK THIS\n",
    "temp=orders_prior_train_df.groupby(\"product_id\")[\"days_since_prior_order\"].mean().reset_index().rename(columns={\"days_since_prior_order\":\"average_dspo_product\"})\n",
    "\n",
    "# Adding the \"average_dspo_product\" feature to orders_prior_train_df\n",
    "orders_prior_train_df=orders_prior_train_df.merge(temp,on=\"product_id\")\n",
    "\n",
    "# Renaming columns to reflect user and product related features\n",
    "# Adding u, p or up prefix to features which tell us if they are user, product or user x product type features\n",
    "orders_prior_train_df=orders_prior_train_df.rename(columns={\"add_to_cart_order\":\"up_add_to_cart_order\",\"order_number\":\"u_order_number\",\"order_dow\":\"u_order_dow\", \"order_hour_of_day\":\"u_order_hour_of_day\",\"days_since_prior_order\":\"u_days_since_prior_order\",\"total_user_purchases\":\"u_total_purchases\",\"number_of_orders\":\"u_number_of_orders\",\"avg_cart_size\":\"u_avg_cart_size\",\"average_days_since_prior_order\":\"u_average_dspo\",\"customer_time\":\"u_customer_time\",\"average_dow\":\"u_average_dow\",\"average_hour_of_day\":\"u_average_hod\",\"total_product_orders\":\"p_total_orders\",\"average_dspo_product\":\"p_average_dspo\"})\n",
    "orders_prior_train_df=orders_prior_train_df.rename(columns={\"average_order_hour_of_day\":\"u_average_hod\"})\n",
    "\n",
    "#Calculating the average add to cart order (\"p_add_to_cart_order\")\n",
    "temp=orders_prior_train_df.groupby(\"product_id\")[\"up_add_to_cart_order\"].mean().reset_index().rename(columns={\"up_add_to_cart_order\":\"p_average_add_to_cart_order\"})\n",
    "\n",
    "# Adding the \"p_add_to_cart_order\" feature to orders_prior_train_df\n",
    "orders_prior_train_df=orders_prior_train_df.merge(temp,on=\"product_id\")\n",
    "\n",
    "#Calculate the number of unique users ordering each product\n",
    "temp=orders_prior_train_df.groupby(\"product_id\")[\"user_id\"].nunique().to_frame(\"p_unique_users\")\n",
    "\n",
    "# Adding the \"p_unique_users\" feature to orders_prior_train_df\n",
    "orders_prior_train_df=orders_prior_train_df.merge(temp,on=\"product_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the orders_prior_train_df dataset for later use\n",
    "orders_prior_train_df.to_pickle(\"orders_prior_train_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will now use this dataset for building user x product features\n",
    "\n",
    "The first set of user x product features which we will build are the so called \"streak_weight\", \"streak\" and \"streak_mod\" features. \n",
    "\n",
    "The streak_weight is a feature which captures the recency of the user-product combination. \n",
    "Let's say a user has 10 orders in total. The user has ordered a particular product in his 10th order, 9th order and 7th order say then\n",
    "    - the streak_weight for the 10th order is (1/2)^(number of orders-order number)=(1/2)^(10-10)=1 (Most recent order)\n",
    "    - the streak_weight for the 9th order is (1/2)^(number of orders-order number) = (1/2)^(10-9)=1/2=0.5\n",
    "    - the streak_weight for the 7th order is (1/2)^(number of orders-order number) = (1/2)^(10-7)=0.125\n",
    "    \n",
    " The \"streak\" feature sums over all the streak-weights for a particular user-product combination. THere is one unique \"streak\" value for a user-product combination. \n",
    " \n",
    " The \"streak_mod\" feature takes the \"streak\" feature and multiplies it with the probability that that particular item is present in the cart which is the \"in_cart_mean\" feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df=pd.read_pickle(\"temp_df.pkl\")\n",
    "temp_df=temp_df[1:]\n",
    "temp_df[\"user_id\"]=temp_df[\"user_id\"].astype(\"uint32\")\n",
    "\n",
    "orders_df=pd.read_csv(\"orders.csv\")\n",
    "\n",
    "orders_df_train=orders_df[orders_df[\"eval_set\"]==\"train\"]\n",
    "\n",
    "temp_df_train=temp_df.merge(orders_df_train,on=\"order_id\",how=\"inner\")\n",
    "temp_df_train=temp_df_train.rename(columns={\"user_id_x\":\"user_id\"}).drop([\"user_id_y\",\"order_number\"],axis=1)\n",
    "temp_df_train.to_pickle(\"temp_df_train_0711.pkl\")\n",
    "\n",
    "temp=orders_prior_df[[\"user_id\",\"u_number_of_orders\"]].drop_duplicates()\n",
    "temp_df_train=temp_df.merge(temp,on=\"user_id\")\n",
    "temp=temp_df_train.groupby(\"user_id\")[\"u_order_number\"].max().reset_index()\n",
    "temp=temp.rename(columns={\"u_order_number\":\"max_order_number_pt\"})\n",
    "temp_df_train=temp_df_train.merge(temp,on=\"user_id\")\n",
    "temp_df_train=temp_df_train[temp_df_train[\"u_order_number\"]<=temp_df_train[\"u_number_of_orders\"]]\n",
    "temp_df_train[\"streak_weight\"]=temp_df_train[\"in_cart\"]*((1/2)**(temp_df_train[\"u_number_of_orders\"]-temp_df[\"u_order_number\"]))\n",
    "temp=temp_df_train.groupby([\"user_id\",\"product_id\"])[\"streak_weight\"].sum().reset_index().rename(columns={\"streak_weight\":\"streak\"})\n",
    "temp_df_train=temp_df_train.merge(temp,on=[\"user_id\",\"product_id\"])\n",
    "temp_df_train=temp_df_train[temp_df[\"streak\"]>0]\n",
    "temp=temp_df_train.groupby([\"user_id\",\"product_id\"])[\"in_cart\"].mean().reset_index().rename(columns={\"in_cart\":\"in_cart_mean\"})\n",
    "temp_df_train=temp_df_train.merge(temp,on=[\"user_id\",\"product_id\"])\n",
    "temp_df_train=temp_df_train[temp_df_train[\"u_order_number\"]==temp_df_train[\"u_number_of_orders\"]]\n",
    "temp_df_train[\"streak_mod\"]=temp_df_train[\"streak\"]*temp_df_train[\"in_cart_mean\"]\n",
    "\n",
    "temp=orders_prior_df[[\"order_id\",\"u_order_dow\",\"u_order_hour_of_day\",\"u_days_since_prior_order\"]].drop_duplicates()\n",
    "temp_df_train=temp_df_train.merge(temp,on=[\"order_id\"])\n",
    "\n",
    "temp=orders_prior_df[[\"user_id\",\"u_total_purchases\",'u_avg_cart_size', 'u_average_dspo',\n",
    "       'u_customer_time', 'u_average_dow', 'average_order_hour_of_day']].drop_duplicates()\n",
    "temp_df_train=temp_df_train.merge(temp,on=\"user_id\")\n",
    "temp_df_train=temp_df_train.rename(columns={\"average_order_hour_of_day\":\"u_average_hod\"})\n",
    "temp_df_train=temp_df_train.drop(\"streak_weight\",axis=1)\n",
    "temp_df_train=temp_df_train.rename(columns={\"u_order_number\":\"up_order_number\",\"streak\":\"up_streak\",\"in_cart_mean\":\"up_in_cart_mean\",\"streak_mod\":\"up_streak_mod\",\"u_order_dow\":\"up_order_dow\",\"u_order_hour_of_day\":\"up_order_hour_of_day\",\"u_days_since_prior_order\":\"up_days_since_prior_order\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the dataset with user and product features\n",
    "temp=orders_prior_df[[\"order_id\",\"u_order_dow\",\"u_order_hour_of_day\",\"u_days_since_prior_order\"]].drop_duplicates()\n",
    "temp_df_train=temp_df.merge(temp,on=[\"order_id\"])\n",
    "temp=orders_prior_df[[\"user_id\",\"u_total_purchases\",'u_avg_cart_size', 'u_average_dspo',\n",
    "       'u_customer_time', 'u_average_dow', 'average_order_hour_of_day']].drop_duplicates()\n",
    "temp_df_train=temp_df_train.merge(temp,on=\"user_id\")\n",
    "temp_df_train=temp_df_train.rename(columns={\"average_order_hour_of_day\":\"u_average_hod\"})\n",
    "temp_df_train=temp_df_train.drop(\"streak_weight\",axis=1)\n",
    "temp_df_train=temp_df_train.rename(columns={\"u_order_number\":\"up_order_number\",\"streak\":\"up_streak\",\"in_cart_mean\":\"up_in_cart_mean\",\"streak_mod\":\"up_streak_mod\",\"u_order_dow\":\"up_order_dow\",\"u_order_hour_of_day\":\"up_order_hour_of_day\",\"u_days_since_prior_order\":\"up_days_since_prior_order\"})\n",
    "prod_features=orders_prior_df[[\"product_id\",\"p_total_orders\",\"p_average_add_to_cart_order\",\"p_unique_users\"]].drop_duplicates()\n",
    "temp_df_train=temp_df_train.merge(prod_features,on=\"product_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_train=temp_df.drop([\"order_id\",\"in_cart\",\"up_order_number\",\"max_order_number_pt\",\"up_order_dow\",\"up_order_hour_of_day\",\"up_days_since_prior_order\"],axis=1)\n",
    "temp_df_train=temp_df_train.merge(orders_df_train,on=\"user_id\")\n",
    "\n",
    "orders_df_test=orders_df[orders_df[\"eval_set\"]==\"test\"]\n",
    "temp_df_test=temp_df_train.merge(orders_df_test,on=\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have our train and test datasets and we are going to pickle them so that we can use them in future without having to run all the pre-processing steps \n",
    "temp_df_train.to_pickle(\"temp_df_train.pkl\")\n",
    "temp_df_test.to_pickle(\"temp_df_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataframe with all prior and train orders, product ids, in_cart \n",
    "prior_train_df=pd.read_pickle(\"temp_df.pkl\")\n",
    "prior_train_df=prior_train_df[1:]\n",
    "orders_df_prior=orders_df[orders_df[\"eval_set\"]==\"prior\"]\n",
    "prior_df= prior_train_df.merge(orders_df_prior[[\"order_id\",\"days_since_prior_order\"]],on=\"order_id\")\n",
    "temp=prior_df.groupby([\"user_id\",\"product_id\"])[\"in_cart\"].sum().reset_index()\n",
    "temp=temp[temp[\"in_cart\"]!=0]\n",
    "temp.rename(columns={\"in_cart\":\"in_cart_sum\"},inplace=True)\n",
    "prior_df=prior_df.merge(temp,on=[\"user_id\",\"product_id\"],how=\"inner\")\n",
    "prior_df.sort_values([\"user_id\",\"product_id\",\"u_order_number\"],inplace=True)\n",
    "prior_df=prior_df.drop(\"index\",axis=1)\n",
    "cumsums=prior_df.groupby([\"user_id\",\"product_id\",\"u_order_number\"]).sum().groupby(level=[0,1]).cumsum().reset_index()\n",
    "cumsums.rename(columns={\"days_since_prior_order\":\"cum_dspo\"},inplace=True)\n",
    "prior_df[\"cum_dspo\"]=cumsums[\"cum_dspo\"]\n",
    "prior_df['order_time_last'] = prior_df[prior_df[\"in_cart\"]==1].groupby([\"user_id\",\"product_id\"])[\"cum_dspo\"].transform(np.max) \n",
    "prior_df_order_time=prior_df.dropna()\n",
    "prior_df_order_time=prior_df_order_time[[\"user_id\",\"product_id\",\"order_time_last\"]].drop_duplicates()\n",
    "prior_df_order_time=prior_df_order_time.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=prior_df.groupby([\"user_id\",\"product_id\"])[\"in_cart\"].sum().reset_index()\n",
    "prior_df_order_time[\"in_cart_sum\"]=temp[\"in_cart\"]\n",
    "prior_df_order_time.drop(\"index\",axis=1,inplace=True)\n",
    "temp_df_train=temp_df_train.merge(prior_df_order_time,on=[\"user_id\",\"product_id\"],how=\"inner\")\n",
    "temp_df_test=temp_df_test.merge(prior_df_order_time,on=[\"user_id\",\"product_id\"],how=\"inner\")\n",
    "temp=prior_df.groupby(\"user_id\")[\"cum_dspo\"].max().reset_index()\n",
    "temp.rename(columns={\"cum_dspo\":\"u_customer_time\"},inplace=True)\n",
    "prior_df=prior_df.drop(\"u_customer_time\",axis=1)\n",
    "\n",
    "temp_df_train.drop(\"u_customer_time\",axis=1,inplace=True)\n",
    "temp_df_test.drop(\"u_customer_time\",axis=1,inplace=True)\n",
    "\n",
    "temp_df_train=temp_df_train.merge(temp,on=\"user_id\")\n",
    "temp_df_test=temp_df_test.merge(temp,on=\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_train=temp_df_train.merge(prior_df_order_time,on=[\"user_id\",\"product_id\"],how=\"inner\")\n",
    "temp_df_test=temp_df_test.merge(prior_df_order_time,on=[\"user_id\",\"product_id\"],how=\"inner\")\n",
    "\n",
    "temp_df_train.rename(columns={\"order_time_last_x\":\"order_time_last\",\"in_cart_sum_x\":\"in_cart_sum\"},inplace=True)\n",
    "temp_df_test.rename(columns={\"order_time_last_x\":\"order_time_last\",\"in_cart_sum_x\":\"in_cart_sum\"},inplace=True)\n",
    "\n",
    "temp_df_train[\"up_freq\"]=temp_df_train[\"u_customer_time\"]/temp_df_train[\"in_cart_sum\"]\n",
    "temp_df_test[\"up_freq\"]=temp_df_test[\"u_customer_time\"]/temp_df_test[\"in_cart_sum\"]\n",
    "\n",
    "temp_df_train[\"up_days_since\"]=temp_df_train[\"u_customer_time\"]-temp_df_train[\"order_time_last\"]+temp_df_train[\"days_since_prior_order\"]\n",
    "temp_df_test[\"up_days_since\"]=temp_df_test[\"u_customer_time\"]-temp_df_test[\"order_time_last\"]+temp_df_test[\"days_since_prior_order\"]\n",
    "\n",
    "temp_df_train[\"up_buildup\"]=temp_df_train[\"up_days_since\"]/temp_df_train[\"up_freq\"]\n",
    "temp_df_test[\"up_buildup\"]=temp_df_test[\"up_days_since\"]/temp_df_test[\"up_freq\"]\n",
    "\n",
    "temp_df_train[temp_df_train[\"up_buildup\"]==np.nan]\n",
    "temp_df_train[temp_df_train.isnull().any(axis=1)]\n",
    "\n",
    "temp_df_train=temp_df_train.fillna(1)\n",
    "temp_df_test=temp_df_test.fillna(1)\n",
    "temp_df_train[temp_df_train.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_train.to_pickle(\"temp_df_train.pkl\")\n",
    "temp_df_test.to_pickle(\"temp_df_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xl_df_train=temp_df_train.drop([\"user_id\",\"order_id\",\"product_id\",\"in_cart\"],axis=1)\n",
    "yl_df_train=temp_df_train[\"in_cart\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a xg boost classifier \n",
    "xg_cl_0719 = xgb.XGBClassifier(objective='binary:logistic', n_estimators=50, max_depth=8, seed=123,scale_pos_weight=4 ,colsample_bytree=0.5)\n",
    "xg_cl_0719.fit(Xl_df_train, yl_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xl_df_test=temp_df_test.drop([\"user_id\",\"order_id\",\"product_id\"],axis=1)\n",
    "# Prediction on test data\n",
    "prediction = xg_cl_0719.predict(Xl_df_test)\n",
    "\n",
    "#Creating .csv file in format which kaggle expects\n",
    "orders_df=pd.read_csv(\"orders.csv\")\n",
    "test_orders_df=orders_df[orders_df[\"eval_set\"]==\"test\"]\n",
    "\n",
    "temp_df_test[\"in_cart\"]=prediction\n",
    "test_set=temp_df_test[temp_df_test[\"in_cart\"]==1]\n",
    "\n",
    "def mylist(df):\n",
    "    ramu=df[\"product_id\"]\n",
    "    if ramu==[]: return 'None'\n",
    "    return ' '.join(str(i) for i in ramu )\n",
    "\n",
    "test_set_list=test_set.groupby(\"order_id\").agg({\"product_id\":(lambda x:list(x))})\n",
    "test_set_list=test_set_list.reset_index()\n",
    "\n",
    "test_set_list['products_list']=test_set_list.apply(mylist,axis=1)\n",
    "\n",
    "kaggle_submission_0719=test_set_list.merge(test_orders_df,on=\"order_id\",how=\"outer\")\n",
    "kaggle_submission_0719=kaggle_submission_0719.fillna('None')\n",
    "kaggle_submission_0719=kaggle_submission_0719[[\"order_id\",\"products_list\"]]\n",
    "kaggle_submission_0719.to_csv(\"final_submission_xgboostcomp_0719.csv\")\n",
    "\n",
    "#We now have the prediction on the test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light GBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-190bc4318458>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mXl_df_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myl_df_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m params = {\n\u001b[0;32m      3\u001b[0m     \u001b[1;34m'task'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;34m'boosting'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'gbdt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;34m'application'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'binary'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lgb' is not defined"
     ]
    }
   ],
   "source": [
    "train = lgb.Dataset(data=Xl_df_train, label=yl_df_train)\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting': 'gbdt',\n",
    "    'application':'binary',\n",
    "    'learning_rate':0.1,\n",
    "    'objective': 'binary',\n",
    "    'metric': {'binary_logloss','auc'},\n",
    "    'num_leaves': 128,#96,128,192,256,512\n",
    "    'max_depth': 12,\n",
    "    'num_threads':4,\n",
    "    'feature_fraction': 0.75,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'bagging_freq': 5,\n",
    "    'lambda_l1':60,\n",
    "    'lambda_l2':30,\n",
    "    'is_unbalance':True,\n",
    "    'verbosity':-1,\n",
    "    'bagging_seed':1294\n",
    "}\n",
    "\n",
    "model=lgb.train(params,train,num_boost_round=446)\n",
    "pred=lgb.predict(Xl_df_test)\n",
    "\n",
    "orders_df=pd.read_csv(\"orders.csv\")\n",
    "ramu=orders_df[orders_df[\"eval_set\"]==\"test\"]\n",
    "def mylist(df):\n",
    "    ramu=df[\"product_id\"]\n",
    "    if ramu==[]: return 'None'\n",
    "    return ' '.join(str(i) for i in ramu )\n",
    "temp_df_test[\"in_cart\"]=pred\n",
    "temp_df_test.head()\n",
    "\n",
    "test_set=temp_df_test[temp_df_test[\"in_cart\"]>0.68]\n",
    "test_set_list=test_set.groupby(\"order_id\").agg({\"product_id\":(lambda x:list(x))})\n",
    "test_set_list.head()\n",
    "test_set_list=test_set_list.reset_index()\n",
    "test_set_list['products_list']=test_set_list.apply(mylist,axis=1)\n",
    "test_set_list.head()\n",
    "sittam=test_set_list.merge(ramu,on=\"order_id\",how=\"outer\")\n",
    "sittam=sittam.fillna('None')\n",
    "sittam=sittam[[\"order_id\",\"products_list\"]]\n",
    "\n",
    "sittam.to_csv(\"final_submission_lgb_0807_v6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
